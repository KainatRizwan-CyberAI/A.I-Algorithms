{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fcf38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Required Libraries\n",
    "# You may need to install these packages first if you haven't already:\n",
    "# pip install pandas scikit-learn matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve\n",
    "import joblib\n",
    "\n",
    "# Step 2: Load and Preprocess Data\n",
    "# Load the dataset (replace 'path_to_dataset.csv' with your dataset path)\n",
    "# Here we use the Kaggle credit card fraud detection dataset\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Step 2.1: Handling Missing Values\n",
    "print(\"Missing values in each column:\\n\", data.isnull().sum())\n",
    "data.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "\n",
    "# Step 2.2: Feature Engineering\n",
    "# Create new features based on existing ones, e.g., transaction amounts, time intervals\n",
    "data['TransactionAmount_log'] = np.log(data['Amount'] + 1)  # Log transformation\n",
    "data['TransactionTime_hour'] = data['Time'] // 3600  # Convert seconds to hours\n",
    "\n",
    "# Step 2.3: Scaling Features\n",
    "features = data[['TransactionAmount_log', 'TransactionTime_hour']]  # Select relevant features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Step 3: Exploratory Data Analysis (EDA)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Class', data=data)\n",
    "plt.title('Distribution of Classes (0: Legitimate, 1: Fraud)')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_scaled, data['Class'], test_size=0.2, random_state=42, stratify=data['Class'])\n",
    "\n",
    "# Step 5: Implement Anomaly Detection Algorithms\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Isolation Forest\": IsolationForest(contamination=0.01, random_state=42),\n",
    "    \"Local Outlier Factor\": LocalOutlierFactor(n_neighbors=20, contamination=0.01),\n",
    "    \"One-Class SVM\": OneClassSVM(kernel='rbf', gamma='auto', nu=0.01)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Fit the model\n",
    "    if model_name == \"Local Outlier Factor\":\n",
    "        model.fit(X_train)  # LOF does not require a predict method in the same way\n",
    "        y_train_pred = model.fit_predict(X_train)\n",
    "        y_test_pred = model.fit_predict(X_test)\n",
    "    else:\n",
    "        model.fit(X_train)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Convert -1 and 1 to 0 and 1 for evaluation\n",
    "    y_train_pred = np.where(y_train_pred == -1, 1, 0)\n",
    "    y_test_pred = np.where(y_test_pred == -1, 1, 0)\n",
    "\n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        \"y_train_pred\": y_train_pred,\n",
    "        \"y_test_pred\": y_test_pred,\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_test_pred),\n",
    "        \"classification_report\": classification_report(y_test, y_test_pred)\n",
    "    }\n",
    "\n",
    "# Step 6: Model Evaluation\n",
    "for model_name, result in results.items():\n",
    "    print(f\"\\n{model_name} - Classification Report:\\n\", result[\"classification_report\"])\n",
    "    print(f\"{model_name} - Confusion Matrix:\\n\", result[\"confusion_matrix\"])\n",
    "\n",
    "# Step 7: ROC and Precision-Recall Curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model_name != \"Local Outlier Factor\":\n",
    "        y_scores = model.decision_function(X_test)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "        roc_auc = roc_auc_score(y_test, y_scores)\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (area = {roc_auc:.2f})')\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Save the Best Model\n",
    "# Save the Isolation Forest model (chosen for this example)\n",
    "joblib.dump(models[\"Isolation Forest\"], 'fraud_detection_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Step 9: Load the Model and Test with New Data (Example)\n",
    "# To load the model later, you can use:\n",
    "# loaded_model = joblib.load('fraud_detection_model.pkl')\n",
    "# loaded_scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# New data example\n",
    "# new_data = pd.DataFrame({'TransactionAmount_log': [2.5], 'TransactionTime_hour': [5]})\n",
    "# new_data_scaled = loaded_scaler.transform(new_data)\n",
    "# prediction = loaded_model.predict(new_data_scaled)\n",
    "# print(f'Prediction: {prediction}')  # -1 for fraud, 1 for legitimate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
